{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedMD-v8-exp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr==0.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox6LThsZqEOp",
        "outputId": "88c44e9f-ef27-4a7d-9745-97686a135e29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr==0.17.0 in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.27.2 in /usr/local/lib/python3.7/dist-packages (from flwr==0.17.0) (1.43.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from flwr==0.17.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.1 in /usr/local/lib/python3.7/dist-packages (from flwr==0.17.0) (3.17.3)\n",
            "Requirement already satisfied: google<3.0.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from flwr==0.17.0) (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from google<3.0.0,>=2.0.3->flwr==0.17.0) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2.0.0,>=1.27.2->flwr==0.17.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f59qPirLpmMI",
        "outputId": "5914a95b-7193-4494-8601-9b5047b0e3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Neural_Networks.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Neural_Networks.py\n",
        "from tensorflow.keras.models import Model, Sequential, clone_model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, add, concatenate, Conv2D,Dropout,\\\n",
        "BatchNormalization, Flatten, MaxPooling2D, AveragePooling2D, Activation, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "  \n",
        "# def cnn_2layer_fc_model(n_classes,n1 = 128, n2=256, dropout_rate = 0.2,input_shape = (28,28)):\n",
        "#    model_A, x = None, None\n",
        "\n",
        "def cnn_2layer_fc_model(n_classes,n1 = 128, n2=256, dropout_rate = 0.2,input_shape = (32, 32, 3)):\n",
        "    model_A, x = None, None\n",
        "    input_shape = (32, 32, 3)\n",
        "    x = Input((32, 32, 3))\n",
        "    if len(input_shape)==2: \n",
        "        y = Reshape((input_shape[0], input_shape[1], 1))(x)\n",
        "    else:\n",
        "        y = Reshape(input_shape)(x)\n",
        "    y = Conv2D(filters = 6, kernel_size = 5)(y)\n",
        "    # y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "    # y = Dropout(dropout_rate)(y)\n",
        "    y = MaxPooling2D(2,2)(y)\n",
        "\n",
        "\n",
        "    y = Conv2D(filters = 16, kernel_size = 5)(y)\n",
        "    # y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "    # y = Dropout(dropout_rate)(y)\n",
        "    #y = AveragePooling2D(pool_size = (2,2), strides = 2, padding = \"valid\")(y)\n",
        "    y = MaxPooling2D(2,2)(y)\n",
        "\n",
        "    y = Flatten()(y)\n",
        "    y = Dense(120)(y)\n",
        "    y = Dense(84)(y)\n",
        "    y = Dense(10)(y)\n",
        "    y = Activation(\"softmax\")(y)\n",
        "\n",
        "\n",
        "    model_A = Model(inputs = x, outputs = y)\n",
        "\n",
        "    model_A.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3), \n",
        "                        loss = \"sparse_categorical_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "    return model_A\n",
        "\n",
        "\n",
        "def remove_last_layer(model, loss = \"mean_absolute_error\"):\n",
        "    \"\"\"\n",
        "    Input: Keras model, a classification model whose last layer is a softmax activation\n",
        "    Output: Keras model, the same model with the last softmax activation layer removed,\n",
        "        while keeping the same parameters \n",
        "    \"\"\"\n",
        "    \n",
        "    new_model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
        "    new_model.set_weights(model.get_weights())\n",
        "    new_model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3), \n",
        "                      loss = loss)\n",
        "    \n",
        "    return new_model\n",
        "\n",
        "\n",
        "\n",
        "def train_models(models, X_train, y_train, X_test, y_test, \n",
        "                 save_dir = \"./\", save_names = None,\n",
        "                 early_stopping = True, min_delta = 0.001, patience = 3, \n",
        "                 batch_size = 128, epochs = 10, is_shuffle=True, verbose = 1\n",
        "                ):\n",
        "    '''\n",
        "    Train an array of models on the same dataset. \n",
        "    We use early termination to speed up training. \n",
        "    '''\n",
        "    \n",
        "    resulting_val_acc = []\n",
        "    record_result = []\n",
        "    for n, model in enumerate(models):\n",
        "        print(\"Training model \", n)\n",
        "        \n",
        "        if early_stopping:\n",
        "            model.fit(X_train, y_train, \n",
        "                      validation_data = [X_test, y_test],\n",
        "                      callbacks=[EarlyStopping(monitor='val_accuracy', min_delta=min_delta, patience=patience)],\n",
        "                      batch_size = batch_size, epochs = epochs, shuffle=is_shuffle, verbose = verbose\n",
        "                     )\n",
        "        else:\n",
        "            model.fit(X_train, y_train, \n",
        "                      validation_data = [X_test, y_test],\n",
        "                      batch_size = batch_size, epochs = epochs, shuffle=is_shuffle, verbose = verbose\n",
        "                     )\n",
        "        '''\n",
        "        model.fit(X_train, y_train, validation_data = [X_test, y_test], \n",
        "                  batch_size = batch_size, epochs = epochs, shuffle=is_shuffle, verbose = verbose)\n",
        "        '''          \n",
        "        resulting_val_acc.append(model.history.history[\"val_accuracy\"][-1])\n",
        "        record_result.append({\"train_acc\": model.history.history[\"accuracy\"], \n",
        "                              \"val_acc\": model.history.history[\"val_accuracy\"],\n",
        "                              \"train_loss\": model.history.history[\"loss\"], \n",
        "                              \"val_loss\": model.history.history[\"val_loss\"]})\n",
        "        \n",
        "        if save_dir is not None:\n",
        "            save_dir_path = os.path.abspath(save_dir)\n",
        "            #make dir\n",
        "            try:\n",
        "                os.makedirs(save_dir_path)\n",
        "            except OSError as e:\n",
        "                if e.errno != errno.EEXIST:\n",
        "                    raise    \n",
        "\n",
        "            if save_names is None:\n",
        "                file_name = save_dir + \"model_{0}\".format(n) + \".h5\"\n",
        "            else:\n",
        "                file_name = save_dir + save_names[n] + \".h5\"\n",
        "            model.save(file_name)\n",
        "    \n",
        "    print(\"pre-train accuracy: \")\n",
        "    print(resulting_val_acc)\n",
        "        \n",
        "    return record_result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_utils.py\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List, Tuple, Union, cast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple\n",
        "import tensorflow as tf\n",
        "import flwr as fl\n",
        "from flwr_experimental.baseline.dataset.dataset import (\n",
        "    XY,\n",
        "    PartitionedDataset,\n",
        "    create_partitioned_dataset,\n",
        "    log_distribution,\n",
        ")\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def generate_alignment_data(X, y, N_alignment = 3000):\n",
        "    \n",
        "    split = StratifiedShuffleSplit(n_splits=1, train_size= N_alignment)\n",
        "    if N_alignment == \"all\":\n",
        "        alignment_data = {}\n",
        "        alignment_data[\"idx\"] = np.arange(y.shape[0])\n",
        "        alignment_data[\"X\"] = X\n",
        "        alignment_data[\"y\"] = y\n",
        "        return alignment_data\n",
        "    for train_index, _ in split.split(X, y):\n",
        "        X_alignment = X[train_index]\n",
        "        y_alignment = y[train_index]\n",
        "    alignment_data = {}\n",
        "    alignment_data[\"idx\"] = train_index\n",
        "    alignment_data[\"X\"] = X_alignment\n",
        "    alignment_data[\"y\"] = y_alignment\n",
        "    \n",
        "    return alignment_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6yRm3__pq6b",
        "outputId": "875a3369-834f-4b6f-a083-0b7a721233df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile FedMD.py\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import clone_model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "from data_utils import generate_alignment_data\n",
        "from Neural_Networks import remove_last_layer\n",
        "\n",
        "class FedMD():\n",
        "    def __init__(self, parties, public_dataset, \n",
        "                 private_data, total_private_data,  \n",
        "                 private_test_data, N_alignment,\n",
        "                 N_rounds, \n",
        "                 N_logits_matching_round, logits_matching_batchsize, \n",
        "                 N_private_training_round, private_training_batchsize):\n",
        "        \n",
        "        self.N_parties = len(parties)\n",
        "        self.public_dataset = public_dataset\n",
        "        self.private_data = private_data\n",
        "        self.private_test_data = private_test_data\n",
        "        self.N_alignment = N_alignment\n",
        "        \n",
        "        self.N_rounds = N_rounds\n",
        "        self.N_logits_matching_round = N_logits_matching_round\n",
        "        self.logits_matching_batchsize = logits_matching_batchsize\n",
        "        self.N_private_training_round = N_private_training_round\n",
        "        self.private_training_batchsize = private_training_batchsize\n",
        "        \n",
        "        self.collaborative_parties = []\n",
        "        self.init_result = []\n",
        "        \n",
        "        print(\"start model initialization: \")\n",
        "        for i in range(self.N_parties):\n",
        "            print(\"model \", i)\n",
        "            model_A_twin = None\n",
        "            model_A_twin = clone_model(parties[i])\n",
        "            model_A_twin.set_weights(parties[i].get_weights())\n",
        "            model_A_twin.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3), \n",
        "                                 loss = \"sparse_categorical_crossentropy\",\n",
        "                                 metrics = [\"accuracy\"])\n",
        "            \n",
        "            print(\"start full stack training ... \")        \n",
        "            \n",
        "            model_A_twin.fit(private_data[i][\"X\"], private_data[i][\"y\"],\n",
        "                             batch_size = 32, epochs = 10, shuffle=True, verbose = 1,\n",
        "                             validation_data = [private_test_data[\"X\"], private_test_data[\"y\"]],\n",
        "                             callbacks=[EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=10)])\n",
        "            \n",
        "            print(\"full stack training done\")\n",
        "            \n",
        "            model_A = remove_last_layer(model_A_twin, loss=\"mean_absolute_error\")\n",
        "            \n",
        "            self.collaborative_parties.append({\"model_logits\": model_A, \n",
        "                                               \"model_classifier\": model_A_twin,\n",
        "                                               \"model_weights\": model_A_twin.get_weights()})\n",
        "            \n",
        "            self.init_result.append({\"val_acc\": model_A_twin.history.history['val_accuracy'],\n",
        "                                     \"train_acc\": model_A_twin.history.history['accuracy'],\n",
        "                                     \"val_loss\": model_A_twin.history.history['val_loss'],\n",
        "                                     \"train_loss\": model_A_twin.history.history['loss'],\n",
        "                                    })\n",
        "            \n",
        "            del model_A, model_A_twin\n",
        "        #END FOR LOOP\n",
        "        '''\n",
        "        print(\"calculate the theoretical upper bounds for participants: \")\n",
        "        \n",
        "        self.upper_bounds = []\n",
        "        self.pooled_train_result = []\n",
        "        for model in parties:\n",
        "            model_ub = clone_model(model)\n",
        "            model_ub.set_weights(model.get_weights())\n",
        "            model_ub.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3),\n",
        "                             loss = \"sparse_categorical_crossentropy\", \n",
        "                             metrics = [\"accuracy\"])\n",
        "            \n",
        "            model_ub.fit(total_private_data[\"X\"], total_private_data[\"y\"],\n",
        "                         batch_size = 32, epochs = 10, shuffle=True, verbose = 1, \n",
        "                         validation_data = [private_test_data[\"X\"], private_test_data[\"y\"]],\n",
        "                         callbacks=[EarlyStopping(monitor=\"val_accuracy\", min_delta=0.001, patience=10)])\n",
        "            \n",
        "            self.upper_bounds.append(model_ub.history.history[\"val_accuracy\"][-1])\n",
        "            self.pooled_train_result.append({\"val_acc\": model_ub.history.history[\"val_accuracy\"], \n",
        "                                             \"acc\": model_ub.history.history[\"accuracy\"]})\n",
        "            \n",
        "            del model_ub    \n",
        "        print(\"the upper bounds are:\", self.upper_bounds)\n",
        "        '''\n",
        "\n",
        "    def collaborative_training(self):\n",
        "        # start collaborating training    \n",
        "        collaboration_performance = {i: [] for i in range(self.N_parties)}\n",
        "        r = 0\n",
        "        while True:\n",
        "            # At beginning of each round, generate new alignment dataset\n",
        "            alignment_data = generate_alignment_data(self.public_dataset[\"X\"], \n",
        "                                                     self.public_dataset[\"y\"],\n",
        "                                                     self.N_alignment)\n",
        "            \n",
        "            print(\"round \", r)\n",
        "            \n",
        "            print(\"update logits ... \")\n",
        "            # update logits\n",
        "            logits = 0\n",
        "            for d in self.collaborative_parties:\n",
        "                d[\"model_logits\"].set_weights(d[\"model_weights\"])\n",
        "                logits += d[\"model_logits\"].predict(alignment_data[\"X\"], verbose = 1)\n",
        "                # print(\"d[model_weights]:\", d[\"model_weights\"])\n",
        "                # print(\"d[model_weights].shape:\", len(d[\"model_weights\"]))\n",
        "                # print(\"alignment_data[X]:\", alignment_data[\"X\"])\n",
        "                # print(\"alignment_data[X].shape:\", alignment_data[\"X\"].shape)\n",
        "                \n",
        "            logits /= self.N_parties\n",
        "\n",
        "            # print(\"logits:\", logits)\n",
        "            # print(\"logits.shape:\", logits.shape)\n",
        "            \n",
        "            # test performance\n",
        "            print(\"test performance ... \")\n",
        "            \n",
        "            for index, d in enumerate(self.collaborative_parties):\n",
        "                y_pred = d[\"model_classifier\"].predict(self.private_test_data[\"X\"], verbose = 0).argmax(axis = 1)\n",
        "                collaboration_performance[index].append(np.mean(self.private_test_data[\"y\"] == y_pred))\n",
        "                \n",
        "                print(collaboration_performance[index][-1])\n",
        "                del y_pred\n",
        "                \n",
        "                \n",
        "            r+= 1\n",
        "            if r > self.N_rounds:\n",
        "                break\n",
        "                \n",
        "                \n",
        "            print(\"updates models ...\")\n",
        "            for index, d in enumerate(self.collaborative_parties):\n",
        "                print(\"model {0} starting alignment with public logits... \".format(index))\n",
        "                \n",
        "                \n",
        "                weights_to_use = None\n",
        "                weights_to_use = d[\"model_weights\"]\n",
        "\n",
        "                d[\"model_logits\"].set_weights(weights_to_use)\n",
        "                d[\"model_logits\"].fit(alignment_data[\"X\"], logits, \n",
        "                                      batch_size = self.logits_matching_batchsize,  \n",
        "                                      epochs = self.N_logits_matching_round, \n",
        "                                      shuffle=True, verbose = 1)\n",
        "                d[\"model_weights\"] = d[\"model_logits\"].get_weights()\n",
        "                print(\"model {0} done alignment\".format(index))\n",
        "\n",
        "                print(\"model {0} starting training with private data... \".format(index))\n",
        "                weights_to_use = None\n",
        "                weights_to_use = d[\"model_weights\"]\n",
        "                d[\"model_classifier\"].set_weights(weights_to_use)\n",
        "                d[\"model_classifier\"].fit(self.private_data[index][\"X\"], \n",
        "                                          self.private_data[index][\"y\"],       \n",
        "                                          batch_size = self.private_training_batchsize, \n",
        "                                          epochs = self.N_private_training_round, \n",
        "                                          shuffle=True, verbose = 1)\n",
        "\n",
        "                d[\"model_weights\"] = d[\"model_classifier\"].get_weights()\n",
        "                print(\"model {0} done private training. \\n\".format(index))\n",
        "            #END FOR LOOP\n",
        "        \n",
        "        #END WHILE LOOP\n",
        "        return collaboration_performance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyW4bywvptNy",
        "outputId": "2ad3dc1f-2638-494c-bf1b-9957ace755bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting FedMD.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CIFAR_noniid.py\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import argparse\n",
        "import sys\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# from data_utils import load_MNIST_data, load_EMNIST_data, generate_EMNIST_writer_based_data, generate_partial_data\n",
        "from FedMD import FedMD\n",
        "from Neural_Networks import train_models, cnn_2layer_fc_model\n",
        "\n",
        "from flwr_experimental.baseline.dataset.dataset import create_partitioned_dataset, XY, XYList, shuffle, sort_by_label_repeating, split_at_fraction, shift, partition, combine_partitions, adjust_xy_shape,sort_by_label\n",
        "\n",
        "\n",
        "def parseArg():\n",
        "    parser = argparse.ArgumentParser(description='FedMD, a federated learning framework. \\\n",
        "    Participants are training collaboratively. ')\n",
        "    parser.add_argument('-conf', metavar='conf_file', nargs=1, \n",
        "                        help='the config file for FedMD.'\n",
        "                       )\n",
        "\n",
        "    conf_file = os.path.abspath(\"conf/EMNIST_imbalance_conf.json\")\n",
        "    \n",
        "    if len(sys.argv) > 1:\n",
        "        args = parser.parse_args(sys.argv[1:])\n",
        "        if args.conf:\n",
        "            conf_file = args.conf[0]\n",
        "    return conf_file\n",
        "\n",
        "CANDIDATE_MODELS = {\"2_layer_CNN\": cnn_2layer_fc_model} \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    conf_file =  parseArg()\n",
        "    with open(conf_file, \"r\") as f:\n",
        "        conf_dict = eval(f.read())\n",
        "        \n",
        "        # n_classes = conf_dict[\"n_classes\"]\n",
        "        model_config = conf_dict[\"models\"]\n",
        "        pre_train_params = conf_dict[\"pre_train_params\"]\n",
        "        model_saved_dir = conf_dict[\"model_saved_dir\"]\n",
        "        model_saved_names = conf_dict[\"model_saved_names\"]\n",
        "        is_early_stopping = conf_dict[\"early_stopping\"]\n",
        "        public_classes = conf_dict[\"public_classes\"]\n",
        "        private_classes = conf_dict[\"private_classes\"]\n",
        "        n_classes = len(public_classes) + len(private_classes)\n",
        "        \n",
        "        emnist_data_dir = conf_dict[\"EMNIST_dir\"]    \n",
        "        N_parties = conf_dict[\"N_parties\"]\n",
        "        N_samples_per_class = conf_dict[\"N_samples_per_class\"]\n",
        "        \n",
        "        N_rounds = conf_dict[\"N_rounds\"]\n",
        "        N_alignment = conf_dict[\"N_alignment\"]\n",
        "        N_private_training_round = conf_dict[\"N_private_training_round\"]\n",
        "        private_training_batchsize = conf_dict[\"private_training_batchsize\"]\n",
        "        N_logits_matching_round = conf_dict[\"N_logits_matching_round\"]\n",
        "        logits_matching_batchsize = conf_dict[\"logits_matching_batchsize\"]\n",
        "        \n",
        "        \n",
        "        result_save_dir = conf_dict[\"result_save_dir\"]\n",
        "\n",
        "    \n",
        "    del conf_dict, conf_file\n",
        "    \n",
        "    (trainset_x, trainset_y), (testset_x, testset_y) = tf.keras.datasets.cifar10.load_data()\n",
        "    # print(trainset_x[0])\n",
        "    global_fraction = 0.2\n",
        "    g_trainset, l_trainset =  (trainset_x[:10000], trainset_y[:10000]), (trainset_x[10000:],trainset_y[10000:])\n",
        "    g_testset, l_testset =  (testset_x[:2000], testset_y[:2000]), (testset_x[2000:],testset_y[2000:])\n",
        "    private_data = (l_trainset, l_testset)\n",
        "    # print(\"local_data\", private_data)\n",
        "\n",
        "    X_train_CIFAR, y_train_CIFAR = g_trainset\n",
        "    X_test_CIFAR, y_test_CIFAR = g_testset\n",
        "    y_train_CIFAR = np.squeeze(y_train_CIFAR)\n",
        "    y_test_CIFAR = np.squeeze(y_test_CIFAR)\n",
        "    public_dataset = {\"X\": X_train_CIFAR, \"y\": y_train_CIFAR}\n",
        "    # print(\"public_dataset[y].shape\", public_dataset[\"y\"].shape)\n",
        "\n",
        "    iid_fraction = 1.0\n",
        "    num_partitions = 10\n",
        "    (l_train_partitions, l_test_partitions), _ = create_partitioned_dataset(private_data, iid_fraction, num_partitions)\n",
        "\n",
        "    private_data = []\n",
        "    total_private_data = {}\n",
        "    total_private_data[\"X\"] = np.array([])\n",
        "    total_private_data[\"y\"] = np.array([])\n",
        "    \n",
        "    private_test_data = {}\n",
        "    private_test_data[\"X\"] = np.array([])\n",
        "    private_test_data[\"y\"] = np.array([])\n",
        "    \n",
        "    # print(\"private_test_data\", private_test_data)\n",
        "    for i in range(num_partitions):\n",
        "        l_x_train, l_y_train = l_train_partitions[i]\n",
        "        l_x_test, l_y_test = l_test_partitions[i]\n",
        "        private_data.append({\"X\": l_x_train, \"y\": l_y_train})\n",
        "        if i == 0:\n",
        "          total_private_data[\"X\"] = l_x_train\n",
        "          total_private_data[\"y\"] = l_y_train\n",
        "          private_test_data[\"X\"] = l_x_test\n",
        "          private_test_data[\"y\"] = l_y_test\n",
        "        # total_private_data[\"X\"].append(l_x_train)\n",
        "        # total_private_data[\"y\"].append(l_y_train)\n",
        "        # private_test_data[\"X\"].append(l_x_test)\n",
        "        # private_test_data[\"y\"].append(l_y_test)\n",
        "        else:\n",
        "          total_private_data[\"X\"] = np.vstack((total_private_data[\"X\"],l_x_train))\n",
        "          total_private_data[\"y\"] = np.hstack((total_private_data[\"y\"],l_y_train))\n",
        "          private_test_data[\"X\"] = np.vstack((private_test_data[\"X\"],l_x_test))\n",
        "          private_test_data[\"y\"] = np.hstack((private_test_data[\"y\"],l_y_test))\n",
        "\n",
        "    # print(\"total_private_data\", total_private_data)\n",
        "    # print(\"private_test_data:\", private_test_data)\n",
        "    # print(\"private_test_data[X].shape:\", private_test_data[\"X\"].shape)\n",
        "    \n",
        "    parties = []\n",
        "    if model_saved_dir is None:\n",
        "        for i, item in enumerate(model_config):\n",
        "            model_name = item[\"model_type\"]\n",
        "            model_params = item[\"params\"]\n",
        "            tmp = CANDIDATE_MODELS[model_name](n_classes=n_classes, \n",
        "                                               input_shape=(28,28),\n",
        "                                               **model_params)\n",
        "            print(\"model {0} : {1}\".format(i, model_saved_names[i]))\n",
        "            print(tmp.summary())\n",
        "            parties.append(tmp)\n",
        "            \n",
        "            del model_name, model_params, tmp\n",
        "        #END FOR LOOP\n",
        "        pre_train_result = train_models(parties, \n",
        "                                        X_train_CIFAR, y_train_CIFAR, \n",
        "                                        X_test_CIFAR, y_test_CIFAR,\n",
        "                                        save_dir = model_saved_dir, save_names = model_saved_names,\n",
        "                                        early_stopping = is_early_stopping,\n",
        "                                        **pre_train_params\n",
        "                                       )\n",
        "    else:\n",
        "        dpath = os.path.abspath(model_saved_dir)\n",
        "        model_names = os.listdir(dpath)\n",
        "        for name in model_names:\n",
        "            tmp = None\n",
        "            tmp = load_model(os.path.join(dpath ,name))\n",
        "            parties.append(tmp)\n",
        "    \n",
        "    \n",
        "    fedmd = FedMD(parties, \n",
        "                  public_dataset = public_dataset,\n",
        "                  private_data = private_data, \n",
        "                  total_private_data = total_private_data,\n",
        "                  private_test_data = private_test_data,\n",
        "                  N_rounds = N_rounds,\n",
        "                  N_alignment = N_alignment, \n",
        "                  N_logits_matching_round = N_logits_matching_round,\n",
        "                  logits_matching_batchsize = logits_matching_batchsize, \n",
        "                  N_private_training_round = N_private_training_round, \n",
        "                  private_training_batchsize = private_training_batchsize)\n",
        "    \n",
        "    initialization_result = fedmd.init_result\n",
        "    # pooled_train_result = fedmd.pooled_train_result\n",
        "    \n",
        "    collaboration_performance = fedmd.collaborative_training()\n",
        "    \n",
        "    if result_save_dir is not None:\n",
        "        save_dir_path = os.path.abspath(result_save_dir)\n",
        "        #make dir\n",
        "        try:\n",
        "            os.makedirs(save_dir_path)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise    \n",
        "    \n",
        "    \n",
        "    with open(os.path.join(save_dir_path, 'pre_train_result.pkl'), 'wb') as f:\n",
        "        pickle.dump(pre_train_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(os.path.join(save_dir_path, 'init_result.pkl'), 'wb') as f:\n",
        "        pickle.dump(initialization_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    # with open(os.path.join(save_dir_path, 'pooled_train_result.pkl'), 'wb') as f:\n",
        "    #     pickle.dump(pooled_train_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(os.path.join(save_dir_path, 'col_performance.pkl'), 'wb') as f:\n",
        "        pickle.dump(collaboration_performance, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dpvobrIp07z",
        "outputId": "ae9a41bc-448f-4c13-ba9d-83c77032fdcf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CIFAR_noniid.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CIFAR_noniid_conf.json\n",
        "{\n",
        "    \"models\": [{\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 256, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 384, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, 'n2': 512, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 256, \"n2\": 256, \"dropout_rate\": 0.3}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 256, \"n2\": 512, \"dropout_rate\": 0.4}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 64, \"n2\": 128, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 64, \"n2\": 128, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 192, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 128, \"dropout_rate\": 0.3}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 128, \"dropout_rate\": 0.3}}\n",
        "              ],\n",
        "    \"pre_train_params\": {\"min_delta\": 0.001, \"patience\": 3,\n",
        "                     \"batch_size\": 128, \"epochs\": 3, \"is_shuffle\": True, \n",
        "                     \"verbose\": 1},\n",
        "    \"model_saved_dir\": None,\n",
        "    \"model_saved_names\" : [\"CNN_128_256\", \"CNN_128_384\", \"CNN_128_512\", \"CNN_256_256\", \"CNN_256_512\", \n",
        "                    \"CNN_64_128_256\", \"CNN_64_128_192\", \"CNN_128_192_256\", \"CNN_128_128_128\", \"CNN_128_128_192\"],\n",
        "    \"early_stopping\" : False,\n",
        "    \"N_parties\": 10,\n",
        "    \"N_samples_per_class\": 3,\n",
        "    \"N_alignment\": 5000, \n",
        "    \"private_classes\": [10, 11, 12, 13, 14, 15], \n",
        "    \"public_classes\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"is_show\": False,\n",
        "    \"N_rounds\": 3,\n",
        "    \"N_logits_matching_round\": 1, \n",
        "    \"N_private_training_round\": 3,\n",
        "    \"private_training_batchsize\" : 5, \n",
        "    \"logits_matching_batchsize\": 256, \n",
        "    \"EMNIST_dir\": \"emnist-letters.mat\",\n",
        "    \"result_save_dir\": \"./result_FEMNIST_imbalanced/\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeN0_H9Ip8ld",
        "outputId": "7e94edc9-3a05-42db-d9eb-0e3b1b677b98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CIFAR_noniid_conf.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python CIFAR_noniid.py -conf CIFAR_noniid_conf.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7CUE8H5qB2W",
        "outputId": "0e042f44-6268-4ba1-f670-f54d887e999e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-12 08:43:09.352567: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "model 0 : CNN_128_256\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 1 : CNN_128_384\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 2 : CNN_128_512\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 14, 14, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 3 : CNN_256_256\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 14, 14, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 4 : CNN_256_512\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 14, 14, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 5 : CNN_64_128_256\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 6 : CNN_64_128_192\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_6 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 7 : CNN_128_192_256\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_7 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 8 : CNN_128_128_128\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_8 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "model 9 : CNN_128_128_192\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " reshape_9 (Reshape)         (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " activation_27 (Activation)  (None, 28, 28, 6)         0         \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " activation_28 (Activation)  (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 5, 5, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            " activation_29 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training model  0\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 8s 90ms/step - loss: 10.9116 - accuracy: 0.1582 - val_loss: 2.6517 - val_accuracy: 0.1970\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 87ms/step - loss: 2.3314 - accuracy: 0.2156 - val_loss: 2.2028 - val_accuracy: 0.2385\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 87ms/step - loss: 2.0384 - accuracy: 0.2716 - val_loss: 2.0537 - val_accuracy: 0.2835\n",
            "Training model  1\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 17.2582 - accuracy: 0.1426 - val_loss: 3.5215 - val_accuracy: 0.1565\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.5910 - accuracy: 0.1746 - val_loss: 2.3875 - val_accuracy: 0.1780\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.2559 - accuracy: 0.1930 - val_loss: 2.2971 - val_accuracy: 0.1870\n",
            "Training model  2\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 8s 90ms/step - loss: 17.4556 - accuracy: 0.1389 - val_loss: 2.6356 - val_accuracy: 0.1615\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 87ms/step - loss: 2.4514 - accuracy: 0.1633 - val_loss: 2.3350 - val_accuracy: 0.1595\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.2787 - accuracy: 0.1721 - val_loss: 2.2872 - val_accuracy: 0.1670\n",
            "Training model  3\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 12.8227 - accuracy: 0.1273 - val_loss: 2.3075 - val_accuracy: 0.1050\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 87ms/step - loss: 2.3062 - accuracy: 0.1047 - val_loss: 2.3061 - val_accuracy: 0.1100\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.3041 - accuracy: 0.1012 - val_loss: 2.3059 - val_accuracy: 0.0960\n",
            "Training model  4\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 8s 91ms/step - loss: 6.9704 - accuracy: 0.1340 - val_loss: 2.3306 - val_accuracy: 0.1350\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.3102 - accuracy: 0.1534 - val_loss: 2.2898 - val_accuracy: 0.1350\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.2611 - accuracy: 0.1548 - val_loss: 2.2711 - val_accuracy: 0.1470\n",
            "Training model  5\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 8.0319 - accuracy: 0.1188 - val_loss: 2.3449 - val_accuracy: 0.1015\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 86ms/step - loss: 2.3332 - accuracy: 0.1143 - val_loss: 2.3186 - val_accuracy: 0.1185\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 86ms/step - loss: 2.3105 - accuracy: 0.1108 - val_loss: 2.3112 - val_accuracy: 0.1010\n",
            "Training model  6\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 7.0290 - accuracy: 0.1209 - val_loss: 2.3225 - val_accuracy: 0.1150\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 84ms/step - loss: 2.3134 - accuracy: 0.1126 - val_loss: 2.3031 - val_accuracy: 0.1160\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 85ms/step - loss: 2.2909 - accuracy: 0.1221 - val_loss: 2.2943 - val_accuracy: 0.1300\n",
            "Training model  7\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 12.2168 - accuracy: 0.1416 - val_loss: 2.5398 - val_accuracy: 0.1705\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 88ms/step - loss: 2.4153 - accuracy: 0.1674 - val_loss: 2.3298 - val_accuracy: 0.1680\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 87ms/step - loss: 2.2929 - accuracy: 0.1767 - val_loss: 2.2826 - val_accuracy: 0.1715\n",
            "Training model  8\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 8s 90ms/step - loss: 30.0638 - accuracy: 0.1063 - val_loss: 2.3089 - val_accuracy: 0.1090\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 2.3045 - accuracy: 0.1011 - val_loss: 2.3061 - val_accuracy: 0.1085\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 2.2994 - accuracy: 0.1022 - val_loss: 2.3129 - val_accuracy: 0.1085\n",
            "Training model  9\n",
            "Epoch 1/3\n",
            "79/79 [==============================] - 8s 91ms/step - loss: 9.0322 - accuracy: 0.1064 - val_loss: 2.3616 - val_accuracy: 0.1060\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 2.3401 - accuracy: 0.1140 - val_loss: 2.3200 - val_accuracy: 0.1060\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 2.2885 - accuracy: 0.1408 - val_loss: 2.2787 - val_accuracy: 0.1385\n",
            "pre-train accuracy: \n",
            "[0.28349998593330383, 0.18700000643730164, 0.16699999570846558, 0.09600000083446503, 0.1469999998807907, 0.10100000351667404, 0.12999999523162842, 0.17149999737739563, 0.10849999636411667, 0.13850000500679016]\n",
            "start model initialization: \n",
            "model  0\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 47ms/step - loss: 2.0801 - accuracy: 0.2533 - val_loss: 1.9971 - val_accuracy: 0.2795\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 1.8535 - accuracy: 0.3298 - val_loss: 1.9500 - val_accuracy: 0.2853\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 1.7072 - accuracy: 0.3765 - val_loss: 1.9603 - val_accuracy: 0.3064\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.6346 - accuracy: 0.4027 - val_loss: 1.9401 - val_accuracy: 0.3114\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.5780 - accuracy: 0.4325 - val_loss: 2.0934 - val_accuracy: 0.3031\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 1.5345 - accuracy: 0.4510 - val_loss: 2.0670 - val_accuracy: 0.3066\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 1.5206 - accuracy: 0.4642 - val_loss: 2.1649 - val_accuracy: 0.2860\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.4570 - accuracy: 0.4762 - val_loss: 2.2920 - val_accuracy: 0.3126\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.3826 - accuracy: 0.5017 - val_loss: 2.0786 - val_accuracy: 0.3201\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.3720 - accuracy: 0.5045 - val_loss: 2.2445 - val_accuracy: 0.3064\n",
            "full stack training done\n",
            "model  1\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 48ms/step - loss: 2.3058 - accuracy: 0.1277 - val_loss: 2.2746 - val_accuracy: 0.1346\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2284 - accuracy: 0.1577 - val_loss: 2.2406 - val_accuracy: 0.1586\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.1565 - accuracy: 0.1970 - val_loss: 2.2422 - val_accuracy: 0.1643\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.0788 - accuracy: 0.2285 - val_loss: 2.1876 - val_accuracy: 0.1946\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.0092 - accuracy: 0.2580 - val_loss: 2.1847 - val_accuracy: 0.2259\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 1.9341 - accuracy: 0.2903 - val_loss: 2.2074 - val_accuracy: 0.2195\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.8639 - accuracy: 0.3122 - val_loss: 2.2269 - val_accuracy: 0.2362\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 1.8112 - accuracy: 0.3447 - val_loss: 2.2124 - val_accuracy: 0.2482\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 49ms/step - loss: 1.7768 - accuracy: 0.3532 - val_loss: 2.2372 - val_accuracy: 0.2686\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 7s 55ms/step - loss: 1.6999 - accuracy: 0.3913 - val_loss: 2.2683 - val_accuracy: 0.2358\n",
            "full stack training done\n",
            "model  2\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 7s 46ms/step - loss: 2.3434 - accuracy: 0.1230 - val_loss: 2.2892 - val_accuracy: 0.1205\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2352 - accuracy: 0.1567 - val_loss: 2.2475 - val_accuracy: 0.1665\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.1374 - accuracy: 0.2080 - val_loss: 2.1580 - val_accuracy: 0.2083\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.0056 - accuracy: 0.2530 - val_loss: 2.0908 - val_accuracy: 0.2271\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.9161 - accuracy: 0.2900 - val_loss: 2.0618 - val_accuracy: 0.2454\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.8273 - accuracy: 0.3140 - val_loss: 2.0882 - val_accuracy: 0.2499\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.7559 - accuracy: 0.3408 - val_loss: 2.1568 - val_accuracy: 0.2444\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.6929 - accuracy: 0.3595 - val_loss: 2.1921 - val_accuracy: 0.2680\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.6328 - accuracy: 0.3918 - val_loss: 2.1699 - val_accuracy: 0.2679\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.6014 - accuracy: 0.4047 - val_loss: 2.3093 - val_accuracy: 0.2514\n",
            "full stack training done\n",
            "model  3\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 9s 66ms/step - loss: 2.3077 - accuracy: 0.0935 - val_loss: 2.3045 - val_accuracy: 0.1009\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.3013 - accuracy: 0.0980 - val_loss: 2.3051 - val_accuracy: 0.1018\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.3019 - accuracy: 0.0845 - val_loss: 2.3213 - val_accuracy: 0.1001\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 8s 64ms/step - loss: 2.3084 - accuracy: 0.0967 - val_loss: 2.3131 - val_accuracy: 0.0981\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.3001 - accuracy: 0.1005 - val_loss: 2.3099 - val_accuracy: 0.1024\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2989 - accuracy: 0.0920 - val_loss: 2.3070 - val_accuracy: 0.0985\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 5s 44ms/step - loss: 2.2987 - accuracy: 0.0978 - val_loss: 2.3052 - val_accuracy: 0.1025\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2999 - accuracy: 0.0953 - val_loss: 2.3079 - val_accuracy: 0.1014\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.3008 - accuracy: 0.1032 - val_loss: 2.3077 - val_accuracy: 0.0980\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.3021 - accuracy: 0.1005 - val_loss: 2.3074 - val_accuracy: 0.1002\n",
            "full stack training done\n",
            "model  4\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.3221 - accuracy: 0.1165 - val_loss: 2.3009 - val_accuracy: 0.1120\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2695 - accuracy: 0.1270 - val_loss: 2.3051 - val_accuracy: 0.1231\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2361 - accuracy: 0.1465 - val_loss: 2.3031 - val_accuracy: 0.1238\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2013 - accuracy: 0.1587 - val_loss: 2.3109 - val_accuracy: 0.1361\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.1742 - accuracy: 0.1865 - val_loss: 2.3018 - val_accuracy: 0.1631\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.1259 - accuracy: 0.2042 - val_loss: 2.3357 - val_accuracy: 0.1564\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.0752 - accuracy: 0.2280 - val_loss: 2.3085 - val_accuracy: 0.1863\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.0398 - accuracy: 0.2368 - val_loss: 2.3728 - val_accuracy: 0.1893\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.0180 - accuracy: 0.2560 - val_loss: 2.3825 - val_accuracy: 0.1771\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.0074 - accuracy: 0.2503 - val_loss: 2.3724 - val_accuracy: 0.1848\n",
            "full stack training done\n",
            "model  5\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.3141 - accuracy: 0.1023 - val_loss: 2.3036 - val_accuracy: 0.0995\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.3055 - accuracy: 0.0993 - val_loss: 2.3083 - val_accuracy: 0.1002\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2982 - accuracy: 0.1013 - val_loss: 2.3095 - val_accuracy: 0.1002\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 2.2980 - accuracy: 0.0997 - val_loss: 2.3048 - val_accuracy: 0.0984\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2962 - accuracy: 0.1082 - val_loss: 2.3223 - val_accuracy: 0.0999\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2991 - accuracy: 0.0997 - val_loss: 2.3055 - val_accuracy: 0.0981\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2971 - accuracy: 0.1042 - val_loss: 2.3242 - val_accuracy: 0.0989\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2979 - accuracy: 0.0980 - val_loss: 2.3239 - val_accuracy: 0.0986\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.2951 - accuracy: 0.1013 - val_loss: 2.3224 - val_accuracy: 0.1023\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.2994 - accuracy: 0.0967 - val_loss: 2.3117 - val_accuracy: 0.1015\n",
            "full stack training done\n",
            "model  6\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 9s 67ms/step - loss: 2.3119 - accuracy: 0.1000 - val_loss: 2.3113 - val_accuracy: 0.1037\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2960 - accuracy: 0.1045 - val_loss: 2.3181 - val_accuracy: 0.1023\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2881 - accuracy: 0.1123 - val_loss: 2.3112 - val_accuracy: 0.1041\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2760 - accuracy: 0.1277 - val_loss: 2.3247 - val_accuracy: 0.1274\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2737 - accuracy: 0.1225 - val_loss: 2.3104 - val_accuracy: 0.1075\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.2732 - accuracy: 0.1203 - val_loss: 2.3233 - val_accuracy: 0.1095\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.2792 - accuracy: 0.1150 - val_loss: 2.3409 - val_accuracy: 0.1105\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 5s 44ms/step - loss: 2.2558 - accuracy: 0.1297 - val_loss: 2.3740 - val_accuracy: 0.1318\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2461 - accuracy: 0.1375 - val_loss: 2.3322 - val_accuracy: 0.1042\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2582 - accuracy: 0.1230 - val_loss: 2.3514 - val_accuracy: 0.1321\n",
            "full stack training done\n",
            "model  7\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.3152 - accuracy: 0.1320 - val_loss: 2.2870 - val_accuracy: 0.1271\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2220 - accuracy: 0.1817 - val_loss: 2.2250 - val_accuracy: 0.1981\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.1587 - accuracy: 0.2142 - val_loss: 2.1922 - val_accuracy: 0.2075\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.0678 - accuracy: 0.2578 - val_loss: 2.2217 - val_accuracy: 0.2201\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.9925 - accuracy: 0.2875 - val_loss: 2.1964 - val_accuracy: 0.2210\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.9504 - accuracy: 0.2988 - val_loss: 2.1988 - val_accuracy: 0.2236\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.8871 - accuracy: 0.3210 - val_loss: 2.2463 - val_accuracy: 0.2243\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 1.8300 - accuracy: 0.3458 - val_loss: 2.2885 - val_accuracy: 0.2184\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 44ms/step - loss: 1.7785 - accuracy: 0.3605 - val_loss: 2.3116 - val_accuracy: 0.2314\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 1.7235 - accuracy: 0.3865 - val_loss: 2.4053 - val_accuracy: 0.2251\n",
            "full stack training done\n",
            "model  8\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 47ms/step - loss: 2.3128 - accuracy: 0.0993 - val_loss: 2.3051 - val_accuracy: 0.1014\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.3004 - accuracy: 0.1035 - val_loss: 2.3049 - val_accuracy: 0.1007\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.2957 - accuracy: 0.0950 - val_loss: 2.3073 - val_accuracy: 0.1004\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2971 - accuracy: 0.0948 - val_loss: 2.3094 - val_accuracy: 0.0997\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 2.3013 - accuracy: 0.1067 - val_loss: 2.3105 - val_accuracy: 0.1021\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2962 - accuracy: 0.0978 - val_loss: 2.3123 - val_accuracy: 0.1004\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2999 - accuracy: 0.0950 - val_loss: 2.3051 - val_accuracy: 0.1001\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.3012 - accuracy: 0.1002 - val_loss: 2.3081 - val_accuracy: 0.1006\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2999 - accuracy: 0.0953 - val_loss: 2.3131 - val_accuracy: 0.0981\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 2.2999 - accuracy: 0.0967 - val_loss: 2.3169 - val_accuracy: 0.1004\n",
            "full stack training done\n",
            "model  9\n",
            "start full stack training ... \n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 6s 43ms/step - loss: 2.3081 - accuracy: 0.1080 - val_loss: 2.3039 - val_accuracy: 0.1030\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2863 - accuracy: 0.1187 - val_loss: 2.3077 - val_accuracy: 0.1119\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2908 - accuracy: 0.1170 - val_loss: 2.3176 - val_accuracy: 0.1088\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2810 - accuracy: 0.1182 - val_loss: 2.3223 - val_accuracy: 0.1101\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2983 - accuracy: 0.1093 - val_loss: 2.3107 - val_accuracy: 0.1024\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2975 - accuracy: 0.1063 - val_loss: 2.3110 - val_accuracy: 0.1020\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2912 - accuracy: 0.1095 - val_loss: 2.3136 - val_accuracy: 0.1026\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2893 - accuracy: 0.1100 - val_loss: 2.3129 - val_accuracy: 0.1037\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2859 - accuracy: 0.1125 - val_loss: 2.3181 - val_accuracy: 0.1039\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 2.2804 - accuracy: 0.1150 - val_loss: 2.3185 - val_accuracy: 0.1031\n",
            "full stack training done\n",
            "round  0\n",
            "update logits ... \n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "test performance ... \n",
            "0.306375\n",
            "0.23575\n",
            "0.251375\n",
            "0.10025\n",
            "0.18475\n",
            "0.1015\n",
            "0.132125\n",
            "0.225125\n",
            "0.100375\n",
            "0.103125\n",
            "updates models ...\n",
            "model 0 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 157ms/step - loss: 0.6754\n",
            "model 0 done alignment\n",
            "model 0 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 8s 9ms/step - loss: 2.1307 - accuracy: 0.2370\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 7s 9ms/step - loss: 2.0281 - accuracy: 0.2598\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 7s 9ms/step - loss: 1.9780 - accuracy: 0.2763\n",
            "model 0 done private training. \n",
            "\n",
            "model 1 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 139ms/step - loss: 0.5066\n",
            "model 1 done alignment\n",
            "model 1 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 7s 9ms/step - loss: 2.2504 - accuracy: 0.1548\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 7s 9ms/step - loss: 2.3071 - accuracy: 0.0945\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 7s 9ms/step - loss: 2.3040 - accuracy: 0.0983\n",
            "model 1 done private training. \n",
            "\n",
            "model 2 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 153ms/step - loss: 0.5605\n",
            "model 2 done alignment\n",
            "model 2 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 9s 12ms/step - loss: 2.2655 - accuracy: 0.1338\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 9s 12ms/step - loss: 2.3074 - accuracy: 0.0948\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3052 - accuracy: 0.0953\n",
            "model 2 done private training. \n",
            "\n",
            "model 3 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 156ms/step - loss: 0.4594\n",
            "model 3 done alignment\n",
            "model 3 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3119 - accuracy: 0.1015\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3052 - accuracy: 0.0930\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3053 - accuracy: 0.0990\n",
            "model 3 done private training. \n",
            "\n",
            "model 4 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.4658\n",
            "model 4 done alignment\n",
            "model 4 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.2767 - accuracy: 0.1400\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 9s 12ms/step - loss: 2.3059 - accuracy: 0.0913\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3048 - accuracy: 0.1000\n",
            "model 4 done private training. \n",
            "\n",
            "model 5 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 158ms/step - loss: 0.4577\n",
            "model 5 done alignment\n",
            "model 5 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3206 - accuracy: 0.0990\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3062 - accuracy: 0.0990\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 9s 11ms/step - loss: 2.3053 - accuracy: 0.0953\n",
            "model 5 done private training. \n",
            "\n",
            "model 6 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 158ms/step - loss: 0.4580\n",
            "model 6 done alignment\n",
            "model 6 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 9s 10ms/step - loss: 2.3088 - accuracy: 0.0938\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.3067 - accuracy: 0.0990\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.3057 - accuracy: 0.0993\n",
            "model 6 done private training. \n",
            "\n",
            "model 7 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 157ms/step - loss: 0.5486\n",
            "model 7 done alignment\n",
            "model 7 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.1831 - accuracy: 0.2077\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.1013 - accuracy: 0.2400\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.0280 - accuracy: 0.2668\n",
            "model 7 done private training. \n",
            "\n",
            "model 8 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 157ms/step - loss: 0.4578\n",
            "model 8 done alignment\n",
            "model 8 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 8s 9ms/step - loss: 2.3107 - accuracy: 0.0980\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 8s 9ms/step - loss: 2.3183 - accuracy: 0.1007\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.3108 - accuracy: 0.0953\n",
            "model 8 done private training. \n",
            "\n",
            "model 9 starting alignment with public logits... \n",
            "20/20 [==============================] - 4s 158ms/step - loss: 0.4570\n",
            "model 9 done alignment\n",
            "model 9 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 8s 9ms/step - loss: 2.3098 - accuracy: 0.0985\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 7s 9ms/step - loss: 2.3137 - accuracy: 0.1010\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 8s 10ms/step - loss: 2.3037 - accuracy: 0.1023\n",
            "model 9 done private training. \n",
            "\n",
            "round  1\n",
            "update logits ... \n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "test performance ... \n",
            "0.227\n",
            "0.100625\n",
            "0.10125\n",
            "0.10075\n",
            "0.098625\n",
            "0.1\n",
            "0.101\n",
            "0.21125\n",
            "0.097875\n",
            "0.099875\n",
            "updates models ...\n",
            "model 0 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.4689\n",
            "model 0 done alignment\n",
            "model 0 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 1.9797 - accuracy: 0.2663\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 1.9199 - accuracy: 0.2960\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 1.9033 - accuracy: 0.3010\n",
            "model 0 done private training. \n",
            "\n",
            "model 1 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1411\n",
            "model 1 done alignment\n",
            "model 1 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3036 - accuracy: 0.0978\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3028 - accuracy: 0.0988\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3042 - accuracy: 0.0967\n",
            "model 1 done private training. \n",
            "\n",
            "model 2 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1447\n",
            "model 2 done alignment\n",
            "model 2 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3054 - accuracy: 0.0957\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3041 - accuracy: 0.0955\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3033 - accuracy: 0.0930\n",
            "model 2 done private training. \n",
            "\n",
            "model 3 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.1417\n",
            "model 3 done alignment\n",
            "model 3 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3038 - accuracy: 0.0983\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3055 - accuracy: 0.0918\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3045 - accuracy: 0.0900\n",
            "model 3 done private training. \n",
            "\n",
            "model 4 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.1516\n",
            "model 4 done alignment\n",
            "model 4 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3035 - accuracy: 0.0978\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3043 - accuracy: 0.0975\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3051 - accuracy: 0.0930\n",
            "model 4 done private training. \n",
            "\n",
            "model 5 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 157ms/step - loss: 0.1416\n",
            "model 5 done alignment\n",
            "model 5 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3048 - accuracy: 0.0920\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3059 - accuracy: 0.0943\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3051 - accuracy: 0.1032\n",
            "model 5 done private training. \n",
            "\n",
            "model 6 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.1444\n",
            "model 6 done alignment\n",
            "model 6 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3329 - accuracy: 0.1050\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3218 - accuracy: 0.1020\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3000 - accuracy: 0.1007\n",
            "model 6 done private training. \n",
            "\n",
            "model 7 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.3091\n",
            "model 7 done alignment\n",
            "model 7 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.0280 - accuracy: 0.2675\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.0158 - accuracy: 0.2760\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 1.9440 - accuracy: 0.3108\n",
            "model 7 done private training. \n",
            "\n",
            "model 8 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.1408\n",
            "model 8 done alignment\n",
            "model 8 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3050 - accuracy: 0.0940\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3028 - accuracy: 0.0903\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3058 - accuracy: 0.0978\n",
            "model 8 done private training. \n",
            "\n",
            "model 9 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.1419\n",
            "model 9 done alignment\n",
            "model 9 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3032 - accuracy: 0.1018\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3039 - accuracy: 0.0930\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3044 - accuracy: 0.1035\n",
            "model 9 done private training. \n",
            "\n",
            "round  2\n",
            "update logits ... \n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "test performance ... \n",
            "0.192375\n",
            "0.098\n",
            "0.10075\n",
            "0.099875\n",
            "0.09725\n",
            "0.099875\n",
            "0.100875\n",
            "0.216625\n",
            "0.100375\n",
            "0.099875\n",
            "updates models ...\n",
            "model 0 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.4460\n",
            "model 0 done alignment\n",
            "model 0 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 5s 7ms/step - loss: 1.9258 - accuracy: 0.3067\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 1.8185 - accuracy: 0.3360\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 1.7929 - accuracy: 0.3465\n",
            "model 0 done private training. \n",
            "\n",
            "model 1 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1340\n",
            "model 1 done alignment\n",
            "model 1 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3019 - accuracy: 0.0978\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3129 - accuracy: 0.1063\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3016 - accuracy: 0.0975\n",
            "model 1 done private training. \n",
            "\n",
            "model 2 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1504\n",
            "model 2 done alignment\n",
            "model 2 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3053 - accuracy: 0.1020\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3019 - accuracy: 0.1013\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 2.3098 - accuracy: 0.0890\n",
            "model 2 done private training. \n",
            "\n",
            "model 3 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1477\n",
            "model 3 done alignment\n",
            "model 3 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3049 - accuracy: 0.0997\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3082 - accuracy: 0.0988\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3079 - accuracy: 0.1020\n",
            "model 3 done private training. \n",
            "\n",
            "model 4 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.1390\n",
            "model 4 done alignment\n",
            "model 4 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 5s 7ms/step - loss: 2.3052 - accuracy: 0.0962\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 5s 7ms/step - loss: 2.3041 - accuracy: 0.0955\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 5s 7ms/step - loss: 2.3036 - accuracy: 0.0943\n",
            "model 4 done private training. \n",
            "\n",
            "model 5 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.1370\n",
            "model 5 done alignment\n",
            "model 5 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3064 - accuracy: 0.0985\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3049 - accuracy: 0.0857\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3041 - accuracy: 0.0965\n",
            "model 5 done private training. \n",
            "\n",
            "model 6 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.1501\n",
            "model 6 done alignment\n",
            "model 6 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3082 - accuracy: 0.1018\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3043 - accuracy: 0.0953\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.2975 - accuracy: 0.0980\n",
            "model 6 done private training. \n",
            "\n",
            "model 7 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.3117\n",
            "model 7 done alignment\n",
            "model 7 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 7s 8ms/step - loss: 2.0531 - accuracy: 0.2510\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 7s 8ms/step - loss: 1.9574 - accuracy: 0.2975\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 7s 8ms/step - loss: 1.8964 - accuracy: 0.3200\n",
            "model 7 done private training. \n",
            "\n",
            "model 8 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1314\n",
            "model 8 done alignment\n",
            "model 8 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3056 - accuracy: 0.1013\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 5s 7ms/step - loss: 2.3049 - accuracy: 0.0965\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3007 - accuracy: 0.0990\n",
            "model 8 done private training. \n",
            "\n",
            "model 9 starting alignment with public logits... \n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.1430\n",
            "model 9 done alignment\n",
            "model 9 starting training with private data... \n",
            "Epoch 1/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3033 - accuracy: 0.1065\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3050 - accuracy: 0.0940\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 6s 7ms/step - loss: 2.3024 - accuracy: 0.1058\n",
            "model 9 done private training. \n",
            "\n",
            "round  3\n",
            "update logits ... \n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "test performance ... \n",
            "0.25875\n",
            "0.09775\n",
            "0.100625\n",
            "0.1005\n",
            "0.09825\n",
            "0.099875\n",
            "0.100125\n",
            "0.231\n",
            "0.10025\n",
            "0.100375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "\n",
        "# 一次性读取\n",
        "f0 = open('result_FEMNIST_imbalanced/pre_train_result.pkl', 'rb')\n",
        "content0 = pkl.load(f0, encoding='latin1')\n",
        "print(\"content0:\", content0)\n",
        "\n",
        "f1 = open('result_FEMNIST_imbalanced/init_result.pkl', 'rb')\n",
        "content1 = pkl.load(f1, encoding='latin1')\n",
        "print(\"content1:\", content1)\n",
        "\n",
        "f2 = open('result_FEMNIST_imbalanced/col_performance.pkl', 'rb')\n",
        "content2 = pkl.load(f2, encoding='latin1')\n",
        "print(\"content2:\", content2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-IwaPRxlI1L",
        "outputId": "781907b9-0cba-459e-b0d1-4b7ecf42abd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content0: [{'train_acc': [0.1581999957561493, 0.21559999883174896, 0.27160000801086426], 'val_acc': [0.19699999690055847, 0.23849999904632568, 0.28349998593330383], 'train_loss': [10.911566734313965, 2.3313913345336914, 2.038372039794922], 'val_loss': [2.6516733169555664, 2.2027699947357178, 2.0537216663360596]}, {'train_acc': [0.14259999990463257, 0.1746000051498413, 0.19300000369548798], 'val_acc': [0.15649999678134918, 0.17800000309944153, 0.18700000643730164], 'train_loss': [17.25820541381836, 2.591010570526123, 2.2558813095092773], 'val_loss': [3.521493434906006, 2.3874807357788086, 2.2971417903900146]}, {'train_acc': [0.1388999968767166, 0.16329999268054962, 0.1720999926328659], 'val_acc': [0.1615000069141388, 0.15950000286102295, 0.16699999570846558], 'train_loss': [17.455598831176758, 2.4514341354370117, 2.278675079345703], 'val_loss': [2.6356492042541504, 2.335031747817993, 2.2871577739715576]}, {'train_acc': [0.12729999423027039, 0.1046999990940094, 0.10119999945163727], 'val_acc': [0.10499999672174454, 0.10999999940395355, 0.09600000083446503], 'train_loss': [12.822662353515625, 2.3062026500701904, 2.304091453552246], 'val_loss': [2.30745267868042, 2.3060553073883057, 2.305943489074707]}, {'train_acc': [0.1340000033378601, 0.1534000039100647, 0.15479999780654907], 'val_acc': [0.13500000536441803, 0.13500000536441803, 0.1469999998807907], 'train_loss': [6.970390796661377, 2.3102357387542725, 2.2610864639282227], 'val_loss': [2.3305869102478027, 2.289760112762451, 2.2711021900177]}, {'train_acc': [0.11879999935626984, 0.11429999768733978, 0.11079999804496765], 'val_acc': [0.1014999970793724, 0.1185000017285347, 0.10100000351667404], 'train_loss': [8.031929016113281, 2.333211898803711, 2.3104867935180664], 'val_loss': [2.3448574542999268, 2.318582773208618, 2.3111565113067627]}, {'train_acc': [0.120899997651577, 0.11259999871253967, 0.12210000306367874], 'val_acc': [0.11500000208616257, 0.11599999666213989, 0.12999999523162842], 'train_loss': [7.029026508331299, 2.3133928775787354, 2.290908098220825], 'val_loss': [2.322460174560547, 2.3030834197998047, 2.294269323348999]}, {'train_acc': [0.14159999787807465, 0.16740000247955322, 0.17669999599456787], 'val_acc': [0.1704999953508377, 0.1679999977350235, 0.17149999737739563], 'train_loss': [12.216793060302734, 2.4153075218200684, 2.292919397354126], 'val_loss': [2.5398144721984863, 2.3298087120056152, 2.282588243484497]}, {'train_acc': [0.1062999963760376, 0.10109999775886536, 0.10220000147819519], 'val_acc': [0.10899999737739563, 0.10849999636411667, 0.10849999636411667], 'train_loss': [30.063846588134766, 2.304539203643799, 2.2994439601898193], 'val_loss': [2.3088576793670654, 2.3060660362243652, 2.31292986869812]}, {'train_acc': [0.10639999806880951, 0.11400000005960464, 0.14079999923706055], 'val_acc': [0.10599999874830246, 0.10599999874830246, 0.13850000500679016], 'train_loss': [9.032155990600586, 2.3401198387145996, 2.2885308265686035], 'val_loss': [2.361577033996582, 2.320005178451538, 2.2786736488342285]}]\n",
            "content1: [{'val_acc': [0.27950000762939453, 0.2852500081062317, 0.30637499690055847, 0.3113749921321869, 0.3031249940395355, 0.30662500858306885, 0.28600001335144043, 0.312624990940094, 0.320125013589859, 0.30637499690055847], 'train_acc': [0.25325000286102295, 0.3297500014305115, 0.3765000104904175, 0.4027499854564667, 0.4325000047683716, 0.45100000500679016, 0.46424999833106995, 0.4762499928474426, 0.5017499923706055, 0.5044999718666077], 'val_loss': [1.9970903396606445, 1.9500459432601929, 1.9603381156921387, 1.940104365348816, 2.0934340953826904, 2.067004442214966, 2.1648659706115723, 2.292043685913086, 2.078613758087158, 2.2444825172424316], 'train_loss': [2.0801167488098145, 1.853505253791809, 1.7071808576583862, 1.634554386138916, 1.5779740810394287, 1.5344942808151245, 1.5205583572387695, 1.4569867849349976, 1.3826491832733154, 1.3720039129257202]}, {'val_acc': [0.13462500274181366, 0.15862500667572021, 0.16425000131130219, 0.19462500512599945, 0.22587500512599945, 0.21950000524520874, 0.23624999821186066, 0.24824999272823334, 0.2686249911785126, 0.2357500046491623], 'train_acc': [0.1277499943971634, 0.1577499955892563, 0.19699999690055847, 0.22849999368190765, 0.257999986410141, 0.2902500033378601, 0.3122499883174896, 0.34474998712539673, 0.35324999690055847, 0.39125001430511475], 'val_loss': [2.2745583057403564, 2.2406444549560547, 2.242180347442627, 2.187575340270996, 2.1846630573272705, 2.2074193954467773, 2.2269186973571777, 2.212357521057129, 2.237245559692383, 2.2683193683624268], 'train_loss': [2.30584454536438, 2.228426933288574, 2.156496286392212, 2.078770637512207, 2.0091733932495117, 1.9341446161270142, 1.863916039466858, 1.8112142086029053, 1.7768077850341797, 1.6998872756958008]}, {'val_acc': [0.12049999833106995, 0.1665000021457672, 0.2082500010728836, 0.22712500393390656, 0.24537500739097595, 0.2498749941587448, 0.24437500536441803, 0.2680000066757202, 0.26787498593330383, 0.2513749897480011], 'train_acc': [0.12300000339746475, 0.15674999356269836, 0.20800000429153442, 0.2529999911785126, 0.28999999165534973, 0.3140000104904175, 0.3407500088214874, 0.359499990940094, 0.3917500078678131, 0.4047499895095825], 'val_loss': [2.289151430130005, 2.247497320175171, 2.157979965209961, 2.090804100036621, 2.061811923980713, 2.088230848312378, 2.156789779663086, 2.192147731781006, 2.169938802719116, 2.3092682361602783], 'train_loss': [2.3434135913848877, 2.2351651191711426, 2.137356996536255, 2.005561351776123, 1.9161286354064941, 1.827309012413025, 1.7559477090835571, 1.692916750907898, 1.632819414138794, 1.601360559463501]}, {'val_acc': [0.10087499767541885, 0.10175000131130219, 0.10012499988079071, 0.09812500327825546, 0.10237500071525574, 0.09849999845027924, 0.10249999910593033, 0.10137499868869781, 0.09799999743700027, 0.1002499982714653], 'train_acc': [0.09350000321865082, 0.09799999743700027, 0.08449999988079071, 0.09674999862909317, 0.10050000250339508, 0.09200000017881393, 0.09775000065565109, 0.09525000303983688, 0.10324999690055847, 0.10050000250339508], 'val_loss': [2.304490804672241, 2.3050553798675537, 2.321280002593994, 2.313061237335205, 2.3098866939544678, 2.3069894313812256, 2.3051817417144775, 2.307884454727173, 2.3076529502868652, 2.307389259338379], 'train_loss': [2.307697057723999, 2.301332950592041, 2.3018903732299805, 2.3084120750427246, 2.3001468181610107, 2.298896551132202, 2.2987077236175537, 2.299879789352417, 2.300825595855713, 2.3020687103271484]}, {'val_acc': [0.1120000034570694, 0.12312500178813934, 0.1237500011920929, 0.13612499833106995, 0.16312499344348907, 0.1563750058412552, 0.1862500011920929, 0.18925000727176666, 0.1771250069141388, 0.1847500056028366], 'train_acc': [0.11649999767541885, 0.12700000405311584, 0.14650000631809235, 0.1587499976158142, 0.18649999797344208, 0.20424999296665192, 0.2280000001192093, 0.23675000667572021, 0.25600001215934753, 0.2502500116825104], 'val_loss': [2.3009276390075684, 2.3051373958587646, 2.303123950958252, 2.310901403427124, 2.301804780960083, 2.335688829421997, 2.308514356613159, 2.3728065490722656, 2.3825185298919678, 2.3724007606506348], 'train_loss': [2.322122573852539, 2.269487142562866, 2.2360544204711914, 2.201322555541992, 2.174208641052246, 2.125933885574341, 2.075185537338257, 2.039754629135132, 2.0179762840270996, 2.0074191093444824]}, {'val_acc': [0.09950000047683716, 0.1002499982714653, 0.1002499982714653, 0.09837500005960464, 0.09987500309944153, 0.09812500327825546, 0.0988750010728836, 0.09862499684095383, 0.10225000232458115, 0.1014999970793724], 'train_acc': [0.10225000232458115, 0.09925000369548798, 0.10125000029802322, 0.09974999725818634, 0.10824999958276749, 0.09974999725818634, 0.1042499989271164, 0.09799999743700027, 0.10125000029802322, 0.09674999862909317], 'val_loss': [2.3035953044891357, 2.30830717086792, 2.309497356414795, 2.3047759532928467, 2.322251796722412, 2.305480718612671, 2.3241512775421143, 2.3239200115203857, 2.3224329948425293, 2.3116614818573], 'train_loss': [2.3140790462493896, 2.3054826259613037, 2.2982356548309326, 2.2980144023895264, 2.29620361328125, 2.299095869064331, 2.2971091270446777, 2.297874927520752, 2.295102834701538, 2.299426794052124]}, {'val_acc': [0.10374999791383743, 0.10225000232458115, 0.1041250005364418, 0.12737500667572021, 0.10750000178813934, 0.10949999839067459, 0.11050000041723251, 0.13175000250339508, 0.1042499989271164, 0.13212500512599945], 'train_acc': [0.10000000149011612, 0.10450000315904617, 0.11225000023841858, 0.1277499943971634, 0.12250000238418579, 0.12025000154972076, 0.11500000208616257, 0.12974999845027924, 0.13750000298023224, 0.12300000339746475], 'val_loss': [2.3112852573394775, 2.318065643310547, 2.3111963272094727, 2.324695110321045, 2.3104238510131836, 2.323317050933838, 2.3408522605895996, 2.3740103244781494, 2.3322317600250244, 2.3514246940612793], 'train_loss': [2.311896800994873, 2.2959792613983154, 2.2880992889404297, 2.276017665863037, 2.273714303970337, 2.273169755935669, 2.2792181968688965, 2.2557528018951416, 2.246055841445923, 2.2582340240478516]}, {'val_acc': [0.12712499499320984, 0.19812500476837158, 0.20749999582767487, 0.2201250046491623, 0.22100000083446503, 0.22362500429153442, 0.22425000369548798, 0.21837499737739563, 0.23137499392032623, 0.2251249998807907], 'train_acc': [0.13199999928474426, 0.18174999952316284, 0.21424999833106995, 0.257750004529953, 0.2874999940395355, 0.29875001311302185, 0.32100000977516174, 0.34575000405311584, 0.3605000078678131, 0.3865000009536743], 'val_loss': [2.2869811058044434, 2.225043773651123, 2.192214012145996, 2.221724033355713, 2.196420907974243, 2.1987619400024414, 2.246267080307007, 2.2885491847991943, 2.311596393585205, 2.405277729034424], 'train_loss': [2.315248489379883, 2.2220354080200195, 2.158658027648926, 2.0678374767303467, 1.9925402402877808, 1.9503536224365234, 1.8871493339538574, 1.8299537897109985, 1.778522253036499, 1.723494529724121]}, {'val_acc': [0.10137499868869781, 0.10074999928474426, 0.10037499666213989, 0.09974999725818634, 0.10212499648332596, 0.10037499666213989, 0.10012499988079071, 0.10062500089406967, 0.09812500327825546, 0.10037499666213989], 'train_acc': [0.09925000369548798, 0.10350000113248825, 0.0949999988079071, 0.09475000202655792, 0.1067499965429306, 0.09775000065565109, 0.0949999988079071, 0.1002499982714653, 0.09525000303983688, 0.09674999862909317], 'val_loss': [2.305056571960449, 2.3049144744873047, 2.3072640895843506, 2.3094351291656494, 2.3105289936065674, 2.3122732639312744, 2.305115222930908, 2.3081307411193848, 2.3131062984466553, 2.3168511390686035], 'train_loss': [2.3127689361572266, 2.3003666400909424, 2.295665740966797, 2.297090530395508, 2.3012681007385254, 2.2961745262145996, 2.2998721599578857, 2.3012375831604004, 2.299916982650757, 2.2998807430267334]}, {'val_acc': [0.10300000011920929, 0.11187499761581421, 0.10875000059604645, 0.11012499779462814, 0.10237500071525574, 0.10199999809265137, 0.10262499749660492, 0.10374999791383743, 0.10387499630451202, 0.10312499850988388], 'train_acc': [0.1080000028014183, 0.11874999850988388, 0.11699999868869781, 0.11824999749660492, 0.10925000160932541, 0.10625000298023224, 0.10949999839067459, 0.10999999940395355, 0.11249999701976776, 0.11500000208616257], 'val_loss': [2.3038532733917236, 2.3077263832092285, 2.31764554977417, 2.3222696781158447, 2.3107376098632812, 2.3110315799713135, 2.313645601272583, 2.3128855228424072, 2.318119525909424, 2.318471908569336], 'train_loss': [2.308138847351074, 2.2862699031829834, 2.290849208831787, 2.2810306549072266, 2.2983181476593018, 2.297461986541748, 2.2912423610687256, 2.289264678955078, 2.2858726978302, 2.2803876399993896]}]\n",
            "content2: {0: [0.306375, 0.227, 0.192375, 0.25875], 1: [0.23575, 0.100625, 0.098, 0.09775], 2: [0.251375, 0.10125, 0.10075, 0.100625], 3: [0.10025, 0.10075, 0.099875, 0.1005], 4: [0.18475, 0.098625, 0.09725, 0.09825], 5: [0.1015, 0.1, 0.099875, 0.099875], 6: [0.132125, 0.101, 0.100875, 0.100125], 7: [0.225125, 0.21125, 0.216625, 0.231], 8: [0.100375, 0.097875, 0.100375, 0.10025], 9: [0.103125, 0.099875, 0.099875, 0.100375]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8qRX4f--nhxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "res_iid100 = [0.112375, 0.227, 0.27875, 0.306375]\n",
        "res_iid75 = [0.10075, 0.198625, 0.248625, 0.262625]\n",
        "res_iid50 = [0.099875, 0.150625, 0.199125, 0.219375]\n",
        "res_iid25 = [0.097875, 0.12975, 0.17675, 0.209875]\n",
        "res_iid10 = [0.09825, 0.140125, 0.15975, 0.207125]\n",
        "\n",
        "rounds = list(range(4))\n",
        "\n",
        "# plt.scatter(rounds, res_iid100)\n",
        "# plt.scatter(rounds, res_iid75)\n",
        "# plt.scatter(rounds, res_iid50)\n",
        "# plt.scatter(rounds, res_iid25)\n",
        "# plt.scatter(rounds, res_iid10)\n",
        "\n",
        "plt.plot(rounds, res_iid100, label=\"iid=1.0\")\n",
        "plt.plot(rounds, res_iid75, label=\"iid=0.75\")\n",
        "plt.plot(rounds, res_iid50, label=\"iid=0.5\")\n",
        "plt.plot(rounds, res_iid25, label=\"iid=0.25\")\n",
        "plt.plot(rounds, res_iid10, label=\"iid=0.1\")\n",
        "plt.xlabel(\"round\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "d7kBVszPlmBK",
        "outputId": "d577837f-d4c8-4e56-dd1a-dc545a3f42d3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}