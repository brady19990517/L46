{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedMD-tf-flower.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile Neural_Networks.py\n",
        "from tensorflow.keras.models import Model, Sequential, clone_model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, add, concatenate, Conv2D,Dropout,\\\n",
        "BatchNormalization, Flatten, MaxPooling2D, AveragePooling2D, Activation, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "  \n",
        "# def cnn_2layer_fc_model(n_classes,n1 = 128, n2=256, dropout_rate = 0.2,input_shape = (28,28)):\n",
        "#    model_A, x = None, None\n",
        "\n",
        "def cnn_2layer_fc_model(n_classes,n1 = 128, n2=256, dropout_rate = 0.2,input_shape = (28,28)):\n",
        "    model_A, x = None, None\n",
        "\n",
        "    x = Input(input_shape)\n",
        "    if len(input_shape)==2: \n",
        "        y = Reshape((input_shape[0], input_shape[1], 1))(x)\n",
        "    else:\n",
        "        y = Reshape(input_shape)(x)\n",
        "    y = Conv2D(filters = 6, kernel_size = 5)(y)\n",
        "    # y = BatchNormalization()(y)\n",
        "    # y = Activation(\"relu\")(y)\n",
        "    # y = Dropout(dropout_rate)(y)\n",
        "    y = MaxPool2D(2,2)\n",
        "\n",
        "\n",
        "    y = Conv2D(filters = 16, kernel_size = 5)(y)\n",
        "    # y = BatchNormalization()(y)\n",
        "    # y = Activation(\"relu\")(y)\n",
        "    # y = Dropout(dropout_rate)(y)\n",
        "    #y = AveragePooling2D(pool_size = (2,2), strides = 2, padding = \"valid\")(y)\n",
        "\n",
        "    y = Flatten()(y)\n",
        "    y = Dense(120)(y)\n",
        "    y = Dense(84)(y)\n",
        "    y = Dense(10)(y)\n",
        "    # y = Activation(\"softmax\")(y)\n",
        "\n",
        "\n",
        "    model_A = Model(inputs = x, outputs = y)\n",
        "\n",
        "    model_A.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3), \n",
        "                        loss = \"sparse_categorical_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "    return model_A\n",
        "\n",
        "\n",
        "def remove_last_layer(model, loss = \"mean_absolute_error\"):\n",
        "    \"\"\"\n",
        "    Input: Keras model, a classification model whose last layer is a softmax activation\n",
        "    Output: Keras model, the same model with the last softmax activation layer removed,\n",
        "        while keeping the same parameters \n",
        "    \"\"\"\n",
        "    \n",
        "    new_model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
        "    new_model.set_weights(model.get_weights())\n",
        "    new_model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3), \n",
        "                      loss = loss)\n",
        "    \n",
        "    return new_model\n",
        "\n",
        "\n",
        "\n",
        "def train_models(models, X_train, y_train, X_test, y_test, \n",
        "                 save_dir = \"./\", save_names = None,\n",
        "                 early_stopping = True, min_delta = 0.001, patience = 3, \n",
        "                 batch_size = 128, epochs = 20, is_shuffle=True, verbose = 1\n",
        "                ):\n",
        "    '''\n",
        "    Train an array of models on the same dataset. \n",
        "    We use early termination to speed up training. \n",
        "    '''\n",
        "    \n",
        "    resulting_val_acc = []\n",
        "    record_result = []\n",
        "    for n, model in enumerate(models):\n",
        "        print(\"Training model \", n)\n",
        "        if early_stopping:\n",
        "            model.fit(X_train, y_train, \n",
        "                      validation_data = [X_test, y_test],\n",
        "                      callbacks=[EarlyStopping(monitor='val_accuracy', min_delta=min_delta, patience=patience)],\n",
        "                      batch_size = batch_size, epochs = epochs, shuffle=is_shuffle, verbose = verbose\n",
        "                     )\n",
        "        else:\n",
        "            model.fit(X_train, y_train, \n",
        "                      validation_data = [X_test, y_test],\n",
        "                      batch_size = batch_size, epochs = epochs, shuffle=is_shuffle, verbose = verbose\n",
        "                     )\n",
        "        \n",
        "        resulting_val_acc.append(model.history.history[\"val_accuracy\"][-1])\n",
        "        record_result.append({\"train_acc\": model.history.history[\"accuracy\"], \n",
        "                              \"val_acc\": model.history.history[\"val_accuracy\"],\n",
        "                              \"train_loss\": model.history.history[\"loss\"], \n",
        "                              \"val_loss\": model.history.history[\"val_loss\"]})\n",
        "        \n",
        "        if save_dir is not None:\n",
        "            save_dir_path = os.path.abspath(save_dir)\n",
        "            #make dir\n",
        "            try:\n",
        "                os.makedirs(save_dir_path)\n",
        "            except OSError as e:\n",
        "                if e.errno != errno.EEXIST:\n",
        "                    raise    \n",
        "\n",
        "            if save_names is None:\n",
        "                file_name = save_dir + \"model_{0}\".format(n) + \".h5\"\n",
        "            else:\n",
        "                file_name = save_dir + save_names[n] + \".h5\"\n",
        "            model.save(file_name)\n",
        "    \n",
        "    print(\"pre-train accuracy: \")\n",
        "    print(resulting_val_acc)\n",
        "        \n",
        "    return record_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrMAkzO9uBAj",
        "outputId": "b6ebb7b7-93b2-49b0-90ed-8db16eeafdcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Neural_Networks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_utils.py\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.keras.datasets import cifar10, cifar100, mnist\n",
        "import scipy.io as sio\n",
        "\n",
        "from flwr_experimental.baseline.dataset.dataset import create_partitioned_dataset, XY, XYList, shuffle, sort_by_label_repeating, split_at_fraction, shift, partition, combine_partitions, adjust_xy_shape, sort_by_label\n",
        "\n",
        "def load_CIFAR_data(standarized = False, verbose = False):    \n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
        "         \n",
        "    y_train = np.squeeze(y_train)\n",
        "    y_test = np.squeeze(y_test)\n",
        "    # substract mean and normalized to [-1/2,1/2]\n",
        "    if standarized: \n",
        "        X_train = X_train/255\n",
        "        X_test = X_test/255\n",
        "        mean_image = np.mean(X_train, axis=0)\n",
        "        X_train -= mean_image\n",
        "        X_test -= mean_image\n",
        "    \n",
        "    if verbose == True: \n",
        "        print(\"X_train shape :\", X_train.shape)\n",
        "        print(\"X_test shape :\", X_test.shape)\n",
        "        print(\"y_train shape :\", y_train.shape)\n",
        "        print(\"y_test shape :\", y_test.shape)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def generate_partial_data(X, y, class_in_use = None, verbose = False):\n",
        "    if class_in_use is None:\n",
        "        idx = np.ones_like(y, dtype = bool)\n",
        "    else:\n",
        "        idx = [y == i for i in class_in_use]\n",
        "        idx = np.any(idx, axis = 0)\n",
        "    X_incomplete, y_incomplete = X[idx], y[idx]\n",
        "    if verbose == True:\n",
        "        print(\"X shape :\", X_incomplete.shape)\n",
        "        print(\"y shape :\", y_incomplete.shape)\n",
        "    return X_incomplete, y_incomplete\n",
        "\n",
        "\n",
        "def generate_alignment_data(X, y, N_alignment = 3000):\n",
        "    \n",
        "    split = StratifiedShuffleSplit(n_splits=1, train_size= N_alignment)\n",
        "    if N_alignment == \"all\":\n",
        "        alignment_data = {}\n",
        "        alignment_data[\"idx\"] = np.arange(y.shape[0])\n",
        "        alignment_data[\"X\"] = X\n",
        "        alignment_data[\"y\"] = y\n",
        "        return alignment_data\n",
        "    for train_index, _ in split.split(X, y):\n",
        "        X_alignment = X[train_index]\n",
        "        y_alignment = y[train_index]\n",
        "    alignment_data = {}\n",
        "    alignment_data[\"idx\"] = train_index\n",
        "    alignment_data[\"X\"] = X_alignment\n",
        "    alignment_data[\"y\"] = y_alignment\n",
        "    \n",
        "    return alignment_data\n",
        "\n",
        "def generate_EMNIST_writer_based_data(X, y, writer_info, N_priv_data_min = 30, \n",
        "                                      N_parties = 5, classes_in_use = range(6)):\n",
        "    \n",
        "    # mask is a boolean array of the same shape as y\n",
        "    # mask[i] = True if y[i] in classes_in_use\n",
        "    mask = None\n",
        "    mask = [y == i for i in classes_in_use]\n",
        "    mask = np.any(mask, axis = 0)\n",
        "    \n",
        "    df_tmp = None\n",
        "    df_tmp = pd.DataFrame({\"writer_ids\": writer_info, \"is_in_use\": mask})\n",
        "    #print(df_tmp.head())\n",
        "    groupped = df_tmp[df_tmp[\"is_in_use\"]].groupby(\"writer_ids\")\n",
        "    \n",
        "    # organize the input the data (X,y) by writer_ids.\n",
        "    # That is, \n",
        "    # data_by_writer is a dictionary where the keys are writer_ids,\n",
        "    # and the contents are the correcponding data. \n",
        "    # Notice that only data with labels in class_in_use are included.\n",
        "    data_by_writer = {}\n",
        "    writer_ids = []\n",
        "    for wt_id, idx in groupped.groups.items():\n",
        "        if len(idx) >= N_priv_data_min:  \n",
        "            writer_ids.append(wt_id)\n",
        "            data_by_writer[wt_id] = {\"X\": X[idx], \"y\": y[idx], \n",
        "                                     \"idx\": idx, \"writer_id\": wt_id}\n",
        "            \n",
        "    # each participant in the collaborative group is assigned data \n",
        "    # from a single writer.\n",
        "    ids_to_use = np.random.choice(writer_ids, size = N_parties, replace = False)\n",
        "    combined_idx = np.array([], dtype = np.int64)\n",
        "    private_data = []\n",
        "    for i in range(N_parties):\n",
        "        id_tmp = ids_to_use[i]\n",
        "        private_data.append(data_by_writer[id_tmp])\n",
        "        combined_idx = np.r_[combined_idx, data_by_writer[id_tmp][\"idx\"]]\n",
        "        del id_tmp\n",
        "    \n",
        "    total_priv_data = {}\n",
        "    total_priv_data[\"idx\"] = combined_idx\n",
        "    total_priv_data[\"X\"] = X[combined_idx]\n",
        "    total_priv_data[\"y\"] = y[combined_idx]\n",
        "    return private_data, total_priv_data\n",
        "\n",
        "\n",
        "def generate_imbal_CIFAR_private_data(X, y, y_super, classes_per_party, N_parties,\n",
        "                                      samples_per_class=7):\n",
        "\n",
        "    priv_data = [None] * N_parties\n",
        "    combined_idxs = []\n",
        "    count = 0\n",
        "    for subcls_list in classes_per_party:\n",
        "        idxs_per_party = []\n",
        "        for c in subcls_list:\n",
        "            idxs = np.flatnonzero(y == c)\n",
        "            idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "            idxs_per_party.append(idxs)\n",
        "        idxs_per_party = np.hstack(idxs_per_party)\n",
        "        combined_idxs.append(idxs_per_party)\n",
        "        \n",
        "        dict_to_add = {}\n",
        "        dict_to_add[\"idx\"] = idxs_per_party\n",
        "        dict_to_add[\"X\"] = X[idxs_per_party]\n",
        "        #dict_to_add[\"y\"] = y[idxs_per_party]\n",
        "        #dict_to_add[\"y_super\"] = y_super[idxs_per_party]\n",
        "        dict_to_add[\"y\"] = y_super[idxs_per_party]\n",
        "        priv_data[count] = dict_to_add\n",
        "        count += 1\n",
        "    \n",
        "    combined_idxs = np.hstack(combined_idxs)\n",
        "    total_priv_data = {}\n",
        "    total_priv_data[\"idx\"] = combined_idxs\n",
        "    total_priv_data[\"X\"] = X[combined_idxs]\n",
        "    #total_priv_data[\"y\"] = y[combined_idxs]\n",
        "    #total_priv_data[\"y_super\"] = y_super[combined_idxs]\n",
        "    total_priv_data[\"y\"] = y_super[combined_idxs]\n",
        "    return priv_data, total_priv_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMvmeIa5uH0P",
        "outputId": "504dcf47-2563-447d-8f3a-81fcfdffc99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile FedMD.py\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import clone_model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "from data_utils import generate_alignment_data\n",
        "from Neural_Networks import remove_last_layer\n",
        "\n",
        "class FedMD():\n",
        "    def __init__(self, parties, public_dataset, \n",
        "                 private_data, total_private_data,  \n",
        "                 private_test_data, N_alignment,\n",
        "                 N_rounds, \n",
        "                 N_logits_matching_round, logits_matching_batchsize, \n",
        "                 N_private_training_round, private_training_batchsize):\n",
        "        \n",
        "        self.N_parties = len(parties)\n",
        "        self.public_dataset = public_dataset\n",
        "        self.private_data = private_data\n",
        "        self.private_test_data = private_test_data\n",
        "        self.N_alignment = N_alignment\n",
        "        \n",
        "        self.N_rounds = N_rounds\n",
        "        self.N_logits_matching_round = N_logits_matching_round\n",
        "        self.logits_matching_batchsize = logits_matching_batchsize\n",
        "        self.N_private_training_round = N_private_training_round\n",
        "        self.private_training_batchsize = private_training_batchsize\n",
        "        \n",
        "        self.collaborative_parties = []\n",
        "        self.init_result = []\n",
        "        \n",
        "        print(\"start model initialization: \")\n",
        "        for i in range(self.N_parties):\n",
        "            print(\"model \", i)\n",
        "            model_A_twin = None\n",
        "            model_A_twin = clone_model(parties[i])\n",
        "            model_A_twin.set_weights(parties[i].get_weights())\n",
        "            model_A_twin.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3), \n",
        "                                 loss = \"sparse_categorical_crossentropy\",\n",
        "                                 metrics = [\"accuracy\"])\n",
        "            \n",
        "            print(\"start full stack training ... \")        \n",
        "            \n",
        "            model_A_twin.fit(private_data[i][\"X\"], private_data[i][\"y\"],\n",
        "                             batch_size = 32, epochs = 25, shuffle=True, verbose = 0,\n",
        "                             validation_data = [private_test_data[\"X\"], private_test_data[\"y\"]],\n",
        "                             callbacks=[EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=10)]\n",
        "                            )\n",
        "            \n",
        "            print(\"full stack training done\")\n",
        "            \n",
        "            model_A = remove_last_layer(model_A_twin, loss=\"mean_absolute_error\")\n",
        "            \n",
        "            self.collaborative_parties.append({\"model_logits\": model_A, \n",
        "                                               \"model_classifier\": model_A_twin,\n",
        "                                               \"model_weights\": model_A_twin.get_weights()})\n",
        "            \n",
        "            self.init_result.append({\"val_acc\": model_A_twin.history.history['val_accuracy'],\n",
        "                                     \"train_acc\": model_A_twin.history.history['accuracy'],\n",
        "                                     \"val_loss\": model_A_twin.history.history['val_loss'],\n",
        "                                     \"train_loss\": model_A_twin.history.history['loss'],\n",
        "                                    })\n",
        "            \n",
        "            print()\n",
        "            del model_A, model_A_twin\n",
        "        #END FOR LOOP\n",
        "        \n",
        "        print(\"calculate the theoretical upper bounds for participants: \")\n",
        "        \n",
        "        self.upper_bounds = []\n",
        "        self.pooled_train_result = []\n",
        "        for model in parties:\n",
        "            model_ub = clone_model(model)\n",
        "            model_ub.set_weights(model.get_weights())\n",
        "            model_ub.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3),\n",
        "                             loss = \"sparse_categorical_crossentropy\", \n",
        "                             metrics = [\"accuracy\"])\n",
        "            \n",
        "            model_ub.fit(total_private_data[\"X\"], total_private_data[\"y\"],\n",
        "                         batch_size = 32, epochs = 50, shuffle=True, verbose = 0, \n",
        "                         validation_data = [private_test_data[\"X\"], private_test_data[\"y\"]],\n",
        "                         callbacks=[EarlyStopping(monitor=\"val_accuracy\", min_delta=0.001, patience=10)])\n",
        "            \n",
        "            self.upper_bounds.append(model_ub.history.history[\"val_accuracy\"][-1])\n",
        "            self.pooled_train_result.append({\"val_acc\": model_ub.history.history[\"val_accuracy\"], \n",
        "                                             \"acc\": model_ub.history.history[\"accuracy\"]})\n",
        "            \n",
        "            del model_ub    \n",
        "        print(\"the upper bounds are:\", self.upper_bounds)\n",
        "    \n",
        "    def collaborative_training(self):\n",
        "        # start collaborating training    \n",
        "        collaboration_performance = {i: [] for i in range(self.N_parties)}\n",
        "        r = 0\n",
        "        while True:\n",
        "            # At beginning of each round, generate new alignment dataset\n",
        "            alignment_data = generate_alignment_data(self.public_dataset[\"X\"], \n",
        "                                                     self.public_dataset[\"y\"],\n",
        "                                                     self.N_alignment)\n",
        "            \n",
        "            print(\"round \", r)\n",
        "            \n",
        "            print(\"update logits ... \")\n",
        "            # update logits\n",
        "            logits = 0\n",
        "            for d in self.collaborative_parties:\n",
        "                d[\"model_logits\"].set_weights(d[\"model_weights\"])\n",
        "                logits += d[\"model_logits\"].predict(alignment_data[\"X\"], verbose = 0)\n",
        "                \n",
        "            logits /= self.N_parties\n",
        "            \n",
        "            # test performance\n",
        "            print(\"test performance ... \")\n",
        "            \n",
        "            for index, d in enumerate(self.collaborative_parties):\n",
        "                y_pred = d[\"model_classifier\"].predict(self.private_test_data[\"X\"], verbose = 0).argmax(axis = 1)\n",
        "                collaboration_performance[index].append(np.mean(self.private_test_data[\"y\"] == y_pred))\n",
        "                \n",
        "                print(collaboration_performance[index][-1])\n",
        "                del y_pred\n",
        "                \n",
        "                \n",
        "            r+= 1\n",
        "            if r > self.N_rounds:\n",
        "                break\n",
        "                \n",
        "                \n",
        "            print(\"updates models ...\")\n",
        "            for index, d in enumerate(self.collaborative_parties):\n",
        "                print(\"model {0} starting alignment with public logits... \".format(index))\n",
        "                \n",
        "                \n",
        "                weights_to_use = None\n",
        "                weights_to_use = d[\"model_weights\"]\n",
        "\n",
        "                d[\"model_logits\"].set_weights(weights_to_use)\n",
        "                d[\"model_logits\"].fit(alignment_data[\"X\"], logits, \n",
        "                                      batch_size = self.logits_matching_batchsize,  \n",
        "                                      epochs = self.N_logits_matching_round, \n",
        "                                      shuffle=True, verbose = 0)\n",
        "                d[\"model_weights\"] = d[\"model_logits\"].get_weights()\n",
        "                print(\"model {0} done alignment\".format(index))\n",
        "\n",
        "                print(\"model {0} starting training with private data... \".format(index))\n",
        "                weights_to_use = None\n",
        "                weights_to_use = d[\"model_weights\"]\n",
        "                d[\"model_classifier\"].set_weights(weights_to_use)\n",
        "                d[\"model_classifier\"].fit(self.private_data[index][\"X\"], \n",
        "                                          self.private_data[index][\"y\"],       \n",
        "                                          batch_size = self.private_training_batchsize, \n",
        "                                          epochs = self.N_private_training_round, \n",
        "                                          shuffle=True, verbose = 0)\n",
        "\n",
        "                d[\"model_weights\"] = d[\"model_classifier\"].get_weights()\n",
        "                print(\"model {0} done private training. \\n\".format(index))\n",
        "            #END FOR LOOP\n",
        "        \n",
        "        #END WHILE LOOP\n",
        "        return collaboration_performance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbuqhrFlt1PK",
        "outputId": "aaae21aa-8c78-4e8b-c9c6-56ba87c7aa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing FedMD.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6ZcJgDstkBS",
        "outputId": "cb8ef42f-b957-4131-c71d-23a3414f2ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing FEMNIST_Imbalanced.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile FEMNIST_Imbalanced.py\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import argparse\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from data_utils import load_MNIST_data, load_EMNIST_data, generate_EMNIST_writer_based_data, generate_partial_data\n",
        "from FedMD import FedMD\n",
        "from Neural_Networks import train_models, cnn_2layer_fc_model, cnn_3layer_fc_model\n",
        "\n",
        "from flwr_experimental.baseline.dataset.dataset import create_partitioned_dataset, XY, XYList, shuffle, sort_by_label_repeating, split_at_fraction, shift, partition, combine_partitions, adjust_xy_shape,sort_by_label\n",
        "\n",
        "\n",
        "def parseArg():\n",
        "    parser = argparse.ArgumentParser(description='FedMD, a federated learning framework. \\\n",
        "    Participants are training collaboratively. ')\n",
        "    parser.add_argument('-conf', metavar='conf_file', nargs=1, \n",
        "                        help='the config file for FedMD.'\n",
        "                       )\n",
        "\n",
        "    conf_file = os.path.abspath(\"conf/EMNIST_imbalance_conf.json\")\n",
        "    \n",
        "    if len(sys.argv) > 1:\n",
        "        args = parser.parse_args(sys.argv[1:])\n",
        "        if args.conf:\n",
        "            conf_file = args.conf[0]\n",
        "    return conf_file\n",
        "\n",
        "CANDIDATE_MODELS = {\"2_layer_CNN\": cnn_2layer_fc_model, \n",
        "                    \"3_layer_CNN\": cnn_3layer_fc_model} \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    conf_file =  parseArg()\n",
        "    with open(conf_file, \"r\") as f:\n",
        "        conf_dict = eval(f.read())\n",
        "        \n",
        "        #n_classes = conf_dict[\"n_classes\"]\n",
        "        model_config = conf_dict[\"models\"]\n",
        "        pre_train_params = conf_dict[\"pre_train_params\"]\n",
        "        model_saved_dir = conf_dict[\"model_saved_dir\"]\n",
        "        model_saved_names = conf_dict[\"model_saved_names\"]\n",
        "        is_early_stopping = conf_dict[\"early_stopping\"]\n",
        "        public_classes = conf_dict[\"public_classes\"]\n",
        "        private_classes = conf_dict[\"private_classes\"]\n",
        "        n_classes = len(public_classes) + len(private_classes)\n",
        "        \n",
        "        emnist_data_dir = conf_dict[\"EMNIST_dir\"]    \n",
        "        N_parties = conf_dict[\"N_parties\"]\n",
        "        N_samples_per_class = conf_dict[\"N_samples_per_class\"]\n",
        "        \n",
        "        N_rounds = conf_dict[\"N_rounds\"]\n",
        "        N_alignment = conf_dict[\"N_alignment\"]\n",
        "        N_private_training_round = conf_dict[\"N_private_training_round\"]\n",
        "        private_training_batchsize = conf_dict[\"private_training_batchsize\"]\n",
        "        N_logits_matching_round = conf_dict[\"N_logits_matching_round\"]\n",
        "        logits_matching_batchsize = conf_dict[\"logits_matching_batchsize\"]\n",
        "        \n",
        "        \n",
        "        result_save_dir = conf_dict[\"result_save_dir\"]\n",
        "\n",
        "    \n",
        "    del conf_dict, conf_file\n",
        "    \n",
        "    X_train_CIFAR, y_train_CIFAR, X_test_CIFAR, y_test_CIFAR \\\n",
        "    = load_CIFAR_data(standarized = True, verbose = True)\n",
        "    \n",
        "    public_dataset = {\"X\": X_train_CIFAR, \"y\": y_train_CIFAR}\n",
        "    \n",
        "    '''''    \n",
        "    X_train_MNIST, y_train_MNIST, X_test_MNIST, y_test_MNIST \\\n",
        "    = load_MNIST_data(standarized = True, verbose = True)\n",
        "    \n",
        "    \n",
        "    X_train_EMNIST, y_train_EMNIST, X_test_EMNIST, y_test_EMNIST, \\\n",
        "    writer_ids_train_EMNIST, writer_ids_test_EMNIST \\\n",
        "    = load_EMNIST_data(emnist_data_dir,\n",
        "                       standarized = True, verbose = True)\n",
        "    \n",
        "\n",
        "    y_train_EMNIST += len(public_classes)\n",
        "    y_test_EMNIST += len(public_classes)\n",
        "\n",
        "    '''''\n",
        "    \n",
        "    #generate private data\n",
        "    private_data, total_private_data\\\n",
        "    =generate_EMNIST_writer_based_data(X_train_EMNIST, y_train_EMNIST,\n",
        "                                       writer_ids_train_EMNIST,\n",
        "                                       N_parties = N_parties, \n",
        "                                       classes_in_use = private_classes, \n",
        "                                       N_priv_data_min = N_samples_per_class * len(private_classes)\n",
        "                                      )\n",
        "    \n",
        "    X_tmp, y_tmp = generate_partial_data(X = X_test_CIFAR, y= y_test_CIFAR, \n",
        "                                         class_in_use = private_classes, verbose = True)\n",
        "    private_test_data = {\"X\": X_tmp, \"y\": y_tmp}\n",
        "    del X_tmp, y_tmp\n",
        "    \n",
        "    parties = []\n",
        "    if model_saved_dir is None:\n",
        "        for i, item in enumerate(model_config):\n",
        "            model_name = item[\"model_type\"]\n",
        "            model_params = item[\"params\"]\n",
        "            tmp = CANDIDATE_MODELS[model_name](n_classes=n_classes, \n",
        "                                               input_shape=(28,28),\n",
        "                                               **model_params)\n",
        "            print(\"model {0} : {1}\".format(i, model_saved_names[i]))\n",
        "            print(tmp.summary())\n",
        "            parties.append(tmp)\n",
        "            \n",
        "            del model_name, model_params, tmp\n",
        "        #END FOR LOOP\n",
        "        pre_train_result = train_models(parties, \n",
        "                                        X_train_MNIST, y_train_MNIST, \n",
        "                                        X_test_MNIST, y_test_MNIST,\n",
        "                                        save_dir = model_saved_dir, save_names = model_saved_names,\n",
        "                                        early_stopping = is_early_stopping,\n",
        "                                        **pre_train_params\n",
        "                                       )\n",
        "    else:\n",
        "        dpath = os.path.abspath(model_saved_dir)\n",
        "        model_names = os.listdir(dpath)\n",
        "        for name in model_names:\n",
        "            tmp = None\n",
        "            tmp = load_model(os.path.join(dpath ,name))\n",
        "            parties.append(tmp)\n",
        "    \n",
        "    del  X_train_MNIST, y_train_MNIST, X_test_MNIST, y_test_MNIST, \\\n",
        "    X_train_EMNIST, y_train_EMNIST, X_test_EMNIST, y_test_EMNIST, writer_ids_train_EMNIST, writer_ids_test_EMNIST\n",
        "    \n",
        "    \n",
        "    fedmd = FedMD(parties, \n",
        "                  public_dataset = public_dataset,\n",
        "                  private_data = private_data, \n",
        "                  total_private_data = total_private_data,\n",
        "                  private_test_data = private_test_data,\n",
        "                  N_rounds = N_rounds,\n",
        "                  N_alignment = N_alignment, \n",
        "                  N_logits_matching_round = N_logits_matching_round,\n",
        "                  logits_matching_batchsize = logits_matching_batchsize, \n",
        "                  N_private_training_round = N_private_training_round, \n",
        "                  private_training_batchsize = private_training_batchsize)\n",
        "    \n",
        "    initialization_result = fedmd.init_result\n",
        "    pooled_train_result = fedmd.pooled_train_result\n",
        "    \n",
        "    collaboration_performance = fedmd.collaborative_training()\n",
        "    \n",
        "    if result_save_dir is not None:\n",
        "        save_dir_path = os.path.abspath(result_save_dir)\n",
        "        #make dir\n",
        "        try:\n",
        "            os.makedirs(save_dir_path)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise    \n",
        "    \n",
        "    \n",
        "    with open(os.path.join(save_dir_path, 'pre_train_result.pkl'), 'wb') as f:\n",
        "        pickle.dump(pre_train_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(os.path.join(save_dir_path, 'init_result.pkl'), 'wb') as f:\n",
        "        pickle.dump(initialization_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(os.path.join(save_dir_path, 'pooled_train_result.pkl'), 'wb') as f:\n",
        "        pickle.dump(pooled_train_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(os.path.join(save_dir_path, 'col_performance.pkl'), 'wb') as f:\n",
        "        pickle.dump(collaboration_performance, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile EMNIST_imbalance_conf.json\n",
        "{\n",
        "    \"models\": [{\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 256, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 384, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 128, 'n2': 512, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 256, \"n2\": 256, \"dropout_rate\": 0.3}},\n",
        "               {\"model_type\": \"2_layer_CNN\", \"params\": {\"n1\": 256, \"n2\": 512, \"dropout_rate\": 0.4}},\n",
        "               {\"model_type\": \"3_layer_CNN\", \"params\": {\"n1\": 64, \"n2\": 128, \"n3\": 256, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"3_layer_CNN\", \"params\": {\"n1\": 64, \"n2\": 128, \"n3\": 192, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"3_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 192, \"n3\": 256, \"dropout_rate\": 0.2}},\n",
        "               {\"model_type\": \"3_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 128, \"n3\": 128, \"dropout_rate\": 0.3}},\n",
        "               {\"model_type\": \"3_layer_CNN\", \"params\": {\"n1\": 128, \"n2\": 128, \"n3\": 192, \"dropout_rate\": 0.3}}\n",
        "              ],\n",
        "    \"pre_train_params\": {\"min_delta\": 0.001, \"patience\": 3,\n",
        "                     \"batch_size\": 128, \"epochs\": 20, \"is_shuffle\": True, \n",
        "                     \"verbose\": 1},\n",
        "    \"model_saved_dir\": None,\n",
        "    \"model_saved_names\" : [\"CNN_128_256\", \"CNN_128_384\", \"CNN_128_512\", \"CNN_256_256\", \"CNN_256_512\", \n",
        "                    \"CNN_64_128_256\", \"CNN_64_128_192\", \"CNN_128_192_256\", \"CNN_128_128_128\", \"CNN_128_128_192\"],\n",
        "    \"early_stopping\" : True,\n",
        "    \"N_parties\": 10,\n",
        "    \"N_samples_per_class\": 3,\n",
        "    \"N_alignment\": 5000, \n",
        "    \"private_classes\": [10, 11, 12, 13, 14, 15], \n",
        "    \"public_classes\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"is_show\": False,\n",
        "    \"N_rounds\": 20,\n",
        "    \"N_logits_matching_round\": 1, \n",
        "    \"N_private_training_round\": 4,\n",
        "    \"private_training_batchsize\" : 5, \n",
        "    \"logits_matching_batchsize\": 256, \n",
        "    \"EMNIST_dir\": \"emnist-letters.mat\",\n",
        "    \"result_save_dir\": \"./result_FEMNIST_imbalanced/\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13LqJ6VOumEg",
        "outputId": "19623379-c074-4c4f-b92b-92ee142f5228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting EMNIST_imbalance_conf.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python FEMNIST_Imbalanced.py -conf EMNIST_imbalance_conf.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_JJgIb1uwpu",
        "outputId": "c5dc8e8c-885a-4881-de7a-570a1311b72e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'FEMNIST_Imbalanced.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    }
  ]
}